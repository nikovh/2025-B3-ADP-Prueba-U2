name: Java CI with Maven and Performance Tests

on:
  push:
    branches: ["main", "develop"]
  pull_request:
    branches: ["main", "develop"]
  workflow_dispatch:

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      checks: write
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up JDK 11
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'
          cache: maven

      - name: Build and test with Maven
        run: mvn -B -ntp clean verify --file proyecto-java/pom.xml

      - name: List target directory contents (Debug)
        if: always()
        run: |
          echo "--- Listing contents of proyecto-java/target ---"
          ls -R proyecto-java/target || true

      - name: Setup k6
        uses: grafana/setup-k6-action@v1

      - name: Run k6 login performance test
        if: always()
        continue-on-error: true
        env:
          BASE_URL: http://localhost:8080
        run: |
          mkdir -p proyecto-java/target
          if [ -z "${BASE_URL}" ]; then
            echo "BASE_URL no definido. k6 se ejecutará sin apuntar a un entorno público."
          fi
          k6 run \
            --summary-export=proyecto-java/target/k6-summary.json \
            pruebas-rendimiento/login-test.js

      - name: Generate Pipeline Dashboard (JUnit + Cucumber + k6)
        if: always()
        run: |
          python3 - << 'PY'
          import json, glob, xml.etree.ElementTree as ET, os

          target = 'proyecto-java/target'
          summary_file = os.environ.get('GITHUB_STEP_SUMMARY', 'summary.txt')

          # -------- JUnit (Surefire) ----------
          tests=failures=errors=skipped=0; total_time=0.0
          for f in glob.glob(f'{target}/surefire-reports/*.xml'):
              try:
                  root = ET.parse(f).getroot()
                  suites = [root] if root.tag=='testsuite' else root.findall('testsuite')
                  for s in suites:
                      tests     += int(s.attrib.get('tests',0))
                      failures  += int(s.attrib.get('failures',0))
                      errors    += int(s.attrib.get('errors',0))
                      skipped   += int(s.attrib.get('skipped',0))
                      total_time+= float(s.attrib.get('time',0))
              except Exception:
                  pass

          # -------- Cucumber (plugin o runner) ----------
          cuc_files = [
              f'{target}/cucumber-reports/Cucumber.json',
              f'{target}/cucumber-reports/cucumber.json',
              f'{target}/cucumber.json',
          ]
          cj = next((p for p in cuc_files if os.path.isfile(p)), None)
          cuc = dict(features=0, sc_total=0, sc_ok=0, sc_ko=0, st_total=0, st_ok=0, st_ko=0, st_sk=0)
          if cj:
              try:
                  arr = json.load(open(cj, encoding='utf-8'))
                  if isinstance(arr, dict) and 'features' in arr:
                      arr = arr['features']
                  if not isinstance(arr, list):
                      arr = [arr]
                  cuc['features'] = len(arr)
                  for feat in arr:
                      for el in feat.get('elements', []):
                          if el.get('type')=='scenario':
                              cuc['sc_total'] += 1
                              step_stats = [ (st.get('result') or {}).get('status') for st in el.get('steps',[]) ]
                              if 'failed' in step_stats: cuc['sc_ko'] += 1
                              else:                      cuc['sc_ok'] += 1
                              for st in el.get('steps', []):
                                  cuc['st_total'] += 1
                                  s = (st.get('result') or {}).get('status')
                                  if s=='passed': cuc['st_ok'] += 1
                                  elif s=='failed': cuc['st_ko'] += 1
                                  else: cuc['st_sk'] += 1
              except Exception:
                  pass

          # -------- k6 (SOLO métricas personalizadas del script) ----------
          k6 = dict(reqs='-', tps='-', p95='-', p99='-', err='-', vus_max='-')
          ks_path = f'{target}/k6-summary.json'
          if os.path.isfile(ks_path):
              try:
                  ks = json.load(open(ks_path, encoding='utf-8'))
                  m  = ks.get('metrics', {})

                  # Trend: login_response_time
                  lrt = m.get('login_response_time', {})
                  vals = (lrt.get('values') or {})
                  percs = lrt.get('percentiles') or {}
                  def v(k): return vals.get(k) or percs.get(k)
                  k6['p95'] = v('p(95)')
                  k6['p99'] = v('p(99)')

                  # Rate: login_checks (éxito); err = 1 - value
                  lch = m.get('login_checks', {})
                  val = lch.get('value')
                  if val is None:
                      val = (lch.get('values') or {}).get('value')
                  if val is not None:
                      k6['err'] = 1.0 - float(val)

                  # Counter: login_iterations (para reqs y TPS)
                  lit = m.get('login_iterations', {})
                  count = lit.get('count')
                  if count is None:
                      count = (lit.get('values') or {}).get('count')
                  if isinstance(count, (int, float)):
                      k6['reqs'] = int(count)
                      dur_ms = (ks.get('state') or {}).get('testRunDurationMs') or 0
                      k6['tps']  = round(count / (dur_ms/1000.0), 2) if dur_ms else '-'

                  # (opcional) VUs máximos si existe
                  vus_max = m.get('vus_max', {})
                  k6['vus_max'] = (vus_max.get('values') or {}).get('value') or vus_max.get('value') or '-'
              except Exception:
                  pass

          lines = []
          lines.append('### Pipeline Dashboard')
          lines.append('')
          lines.append('#### Pruebas funcionales (JUnit)')
          lines.append(f'- Tests: {tests} | Failures: {failures} | Errors: {errors} | Skipped: {skipped} | Tiempo: {total_time:.2f}s')
          lines.append('')
          lines.append('#### Cucumber (BDD)')
          lines.append(f"- Features: {cuc['features']} | Escenarios: {cuc['sc_total']} (OK {cuc['sc_ok']}, Fallidos {cuc['sc_ko']})")
          lines.append(f"- Steps: {cuc['st_total']} (OK {cuc['st_ok']}, Fallidos {cuc['st_ko']}, Skipped {cuc['st_sk']})")
          lines.append('')
          lines.append('#### Performance (k6 – simulación de login)')
          lines.append(f"- Iteraciones: {k6['reqs']} | TPS aprox: {k6['tps']} iter/s")
          lines.append(f"- Latencia p95: {k6['p95']} ms | p99: {k6['p99']} ms")
          lines.append(f"- Tasa de Error: {k6['err']}")
          lines.append(f"- VUs max: {k6['vus_max']}")
          out = '\n'.join(lines)
          with open(summary_file, 'a', encoding='utf-8') as f:
              f.write(out + '\n')
          PY

      # ---------- Artifacts y publicación (cada paso con "uses", NO dentro de run) ----------
      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: surefire-reports
          path: proyecto-java/target/surefire-reports/**
          if-no-files-found: ignore
          retention-days: 7

      - name: Upload Cucumber report (HTML + JSON)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cucumber-reports
          path: |
            proyecto-java/target/cucumber-report.html
            proyecto-java/target/cucumber.json
            proyecto-java/target/cucumber-html-reports/**
            proyecto-java/target/cucumber-reports/**
          if-no-files-found: ignore
          retention-days: 7

      - name: Upload k6 summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: k6-summary
          path: proyecto-java/target/k6-summary.json
          if-no-files-found: ignore
          retention-days: 7

      - name: Publish Unit Test Results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: proyecto-java/target/surefire-reports/TEST-*.xml

      - name: Upload built JAR
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: app-jar
          path: proyecto-java/target/*.jar
          if-no-files-found: ignore
          retention-days: 7